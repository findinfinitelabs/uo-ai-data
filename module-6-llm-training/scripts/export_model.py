#!/usr/bin/env python3
"""
Export Fine-Tuned Model for Deployment

This script exports your fine-tuned Mistral 7B model to various formats:
- Merged model (LoRA weights merged with base model)
- GGUF format (for use with Ollama, llama.cpp)
- Hugging Face Hub upload
"""

import argparse
import subprocess
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def merge_lora_weights(
    base_model: str,
    adapter_path: str,
    output_path: str,
    cache_dir: str = "./models"
):
    """
    Merge LoRA adapter weights with the base model.
    
    Args:
        base_model: Base model name on Hugging Face
        adapter_path: Path to LoRA adapters
        output_path: Where to save merged model
        cache_dir: Model cache directory
    """
    print("\nüì¶ Loading base model...")
    
    # Load base model (without quantization for merging)
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        cache_dir=cache_dir,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        base_model,
        cache_dir=cache_dir,
        trust_remote_code=True,
    )
    
    print(f"üîó Loading LoRA adapters from: {adapter_path}")
    model = PeftModel.from_pretrained(model, adapter_path)
    
    print("üîÑ Merging weights...")
    model = model.merge_and_unload()
    
    print(f"üíæ Saving merged model to: {output_path}")
    Path(output_path).mkdir(parents=True, exist_ok=True)
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    print("‚úÖ Model merged and saved successfully!")
    return output_path


def convert_to_gguf(
    model_path: str,
    output_path: str,
    quantization: str = "q4_k_m"
):
    """
    Convert model to GGUF format for Ollama/llama.cpp.
    
    Args:
        model_path: Path to merged model
        output_path: Where to save GGUF file
        quantization: Quantization type (q4_k_m, q5_k_m, q8_0, f16)
    """
    print(f"\nüîÑ Converting to GGUF format (quantization: {quantization})")
    
    # Check if llama.cpp conversion script exists
    convert_script = Path.home() / "llama.cpp" / "convert_hf_to_gguf.py"
    
    if not convert_script.exists():
        print("\n‚ö†Ô∏è  llama.cpp not found. To convert to GGUF, you need to:")
        print("   1. Clone llama.cpp: git clone https://github.com/ggerganov/llama.cpp")
        print("   2. Install requirements: pip install -r llama.cpp/requirements.txt")
        print("   3. Run the conversion manually:")
        print(f"      python llama.cpp/convert_hf_to_gguf.py {model_path} --outfile {output_path}")
        print(f"      llama.cpp/llama-quantize {output_path} {output_path.replace('.gguf', f'-{quantization}.gguf')} {quantization}")
        return None
    
    # Convert to GGUF
    gguf_output = Path(output_path) / "model.gguf"
    
    cmd = [
        "python", str(convert_script),
        model_path,
        "--outfile", str(gguf_output)
    ]
    
    print(f"   Running: {' '.join(cmd)}")
    subprocess.run(cmd, check=True)
    
    # Quantize
    if quantization != "f16":
        quantize_script = Path.home() / "llama.cpp" / "llama-quantize"
        quantized_output = Path(output_path) / f"model-{quantization}.gguf"
        
        cmd = [
            str(quantize_script),
            str(gguf_output),
            str(quantized_output),
            quantization
        ]
        
        print(f"   Running: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        print(f"‚úÖ GGUF file saved to: {quantized_output}")
        return str(quantized_output)
    
    print(f"‚úÖ GGUF file saved to: {gguf_output}")
    return str(gguf_output)


def create_ollama_modelfile(
    gguf_path: str,
    model_name: str,
    output_path: str
):
    """
    Create an Ollama Modelfile for the converted model.
    
    Args:
        gguf_path: Path to GGUF model file
        model_name: Name for the Ollama model
        output_path: Where to save the Modelfile
    """
    modelfile_content = f"""# Modelfile for {model_name}
# Generated by export_model.py

FROM {gguf_path}

# Model parameters
PARAMETER temperature 0.7
PARAMETER top_p 0.95
PARAMETER top_k 50

# System prompt for data management tasks
SYSTEM \"\"\"
You are an AI assistant specialized in data management and generation tasks.
You help with:
- Generating synthetic data that follows real-world patterns
- Converting natural language to SQL queries
- Validating data quality and identifying issues
- Creating data documentation and dictionaries

Always provide accurate, well-structured responses.
\"\"\"

# Template for Mistral instruction format
TEMPLATE \"\"\"<s>[INST] {{{{ .System }}}}

{{{{ .Prompt }}}} [/INST]
{{{{ .Response }}}}\"\"\"
"""
    
    modelfile_path = Path(output_path) / "Modelfile"
    with open(modelfile_path, 'w') as f:
        f.write(modelfile_content)
    
    print(f"\n‚úÖ Modelfile created: {modelfile_path}")
    print(f"\nTo import into Ollama, run:")
    print(f"   ollama create {model_name} -f {modelfile_path}")


def push_to_hub(
    model_path: str,
    repo_name: str,
    private: bool = True
):
    """
    Push model to Hugging Face Hub.
    
    Args:
        model_path: Path to merged model
        repo_name: Repository name (e.g., "username/model-name")
        private: Whether to make the repo private
    """
    from huggingface_hub import HfApi, login
    
    print(f"\nüì§ Pushing model to Hugging Face Hub: {repo_name}")
    
    # Login to Hugging Face
    login()
    
    api = HfApi()
    
    # Create repo if it doesn't exist
    api.create_repo(
        repo_id=repo_name,
        private=private,
        exist_ok=True,
    )
    
    # Upload model
    api.upload_folder(
        folder_path=model_path,
        repo_id=repo_name,
        repo_type="model",
    )
    
    print(f"‚úÖ Model pushed to: https://huggingface.co/{repo_name}")


def main():
    parser = argparse.ArgumentParser(
        description="Export fine-tuned Mistral 7B model"
    )
    parser.add_argument(
        "--base-model",
        type=str,
        default="mistralai/Mistral-7B-Instruct-v0.2",
        help="Base model name"
    )
    parser.add_argument(
        "--adapter",
        type=str,
        default="./models/mistral-7b-finetuned",
        help="Path to LoRA adapter weights"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./models/mistral-7b-merged",
        help="Output directory for merged model"
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default="./models",
        help="Model cache directory"
    )
    parser.add_argument(
        "--format",
        type=str,
        choices=["merged", "gguf", "hub", "all"],
        default="merged",
        help="Export format"
    )
    parser.add_argument(
        "--quantization",
        type=str,
        default="q4_k_m",
        choices=["q4_k_m", "q5_k_m", "q8_0", "f16"],
        help="GGUF quantization type"
    )
    parser.add_argument(
        "--hub-repo",
        type=str,
        help="Hugging Face Hub repository name"
    )
    parser.add_argument(
        "--ollama-name",
        type=str,
        default="mistral-data-gen",
        help="Name for Ollama model"
    )
    
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("üì¶ MODEL EXPORT")
    print("=" * 60)
    
    merged_path = args.output
    
    # Always merge first
    if args.format in ["merged", "gguf", "hub", "all"]:
        merged_path = merge_lora_weights(
            base_model=args.base_model,
            adapter_path=args.adapter,
            output_path=args.output,
            cache_dir=args.cache_dir,
        )
    
    # Convert to GGUF if requested
    if args.format in ["gguf", "all"]:
        gguf_path = convert_to_gguf(
            model_path=merged_path,
            output_path=args.output + "-gguf",
            quantization=args.quantization,
        )
        
        if gguf_path:
            create_ollama_modelfile(
                gguf_path=gguf_path,
                model_name=args.ollama_name,
                output_path=args.output + "-gguf",
            )
    
    # Push to Hub if requested
    if args.format in ["hub", "all"]:
        if not args.hub_repo:
            print("\n‚ö†Ô∏è  --hub-repo is required for Hub upload")
        else:
            push_to_hub(
                model_path=merged_path,
                repo_name=args.hub_repo,
            )
    
    print("\n" + "=" * 60)
    print("‚úÖ EXPORT COMPLETE!")
    print("=" * 60)


if __name__ == "__main__":
    main()
